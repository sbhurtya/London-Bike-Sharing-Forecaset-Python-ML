{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import holidays\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Save requirements\n",
    "os.system(\"pip freeze > requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/london_merged.csv')\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metadata:\n",
    "  - \"timestamp\" - timestamp field for grouping the data\n",
    "  - \"cnt\" - the count of a new bike shares\n",
    "  - \"t1\" - real temperature in C\n",
    "  - \"t2\" - temperature in C \"feels like\"\n",
    "  - \"hum\" - humidity in percentage\n",
    "  - \"wind_speed\" - wind speed in km/h\n",
    "  - \"weather_code\" - category of the weather\n",
    "  - \"is_holiday\" - boolean field - 1 holiday / 0 non holiday\n",
    "  - \"is_weekend\" - boolean field - 1 if the day is weekend\n",
    "  - \"season\" - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter.\n",
    "  - \"weathe_code\" category description:\n",
    "     - 1 = Clear ; mostly clear but have some values with haze/fog/patches of fog/ fog in vicinity \n",
    "     - 2 = scattered clouds / few clouds \n",
    "     - 3 = Broken clouds \n",
    "     - 4 = Cloudy \n",
    "     - 7 = Rain/ light Rain shower/ Light rain \n",
    "     - 10 = rain with thunderstorm \n",
    "     - 26 = snowfall \n",
    "     - 94 = Freezing Fog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "#Keep records from 2015 and 2016\n",
    "df = df[(df['timestamp'].dt.year == 2015) | (df['timestamp'].dt.year == 2016)]\n",
    "#Sort the values by timestamp\n",
    "df = df.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No missing values. But there might be missing timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing timestamps\n",
    "all_days = pd.date_range(start=df['timestamp'].min(), end=df['timestamp'].max(), freq='h')\n",
    "missing_days = all_days[~all_days.isin(df['timestamp'])]\n",
    "print('Number of missing timestamps:', len(missing_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 130 timestamps are missing. We will imput them using existing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#London holidays\n",
    "uk_holidays = holidays.UK(years=[df['timestamp'].dt.year.min(), df['timestamp'].dt.year.max()])\n",
    "uk_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataframe using all days\n",
    "df_full = pd.DataFrame(all_days, columns=['timestamp'])\n",
    "#Merge with df to get cnt, t1, t2, hum, wind_speed, weather_code, season\n",
    "df_full = df_full.merge(df[['timestamp', 'cnt', 't1', 't2', 'hum', 'wind_speed', 'weather_code', 'season']], on='timestamp', how='left')\n",
    "#is_holiday column: 1 if holiday, 0 if not\n",
    "df_full['is_holiday'] = np.where(df_full['timestamp'].dt.date.isin(uk_holidays), 1, 0)\n",
    "df_full['is_weekend'] = np.where(df_full['timestamp'].dt.dayofweek.isin([5, 6]), 1, 0)\n",
    "\n",
    "#Backfill missing values\n",
    "df_full = df_full.ffill()\n",
    "df = df_full.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days = all_days[~all_days.isin(df['timestamp'])]\n",
    "print('Number of missing timestamps:', len(missing_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the timestamp as the index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "#Set period to 1 hour\n",
    "df.index = pd.DatetimeIndex(df.index).to_period('h')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample('D').agg({'cnt':'sum', \n",
    "                           't1':'median', \n",
    "                           't2':'median', \n",
    "                           'hum':'median', \n",
    "                           'wind_speed':'median', \n",
    "                           'weather_code': lambda x: x.value_counts().index[0], \n",
    "                           'season': lambda x: x.value_counts().index[0], \n",
    "                           'is_holiday':'max', \n",
    "                           'is_weekend':'max'})\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Boxplot of all the columns\n",
    "plt.figure(figsize=(10, 12))\n",
    "cols = df.columns\n",
    "print(cols)\n",
    "for i in range(1, len(cols)):\n",
    "    print(cols[i])\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(df[cols[i-1]])\n",
    "    plt.title(cols[i-1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no abnormal data in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Real and feels like temperature are highly correlated. Let's use feels like temperature since it is more likely to impact the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop t1\n",
    "df.drop(['t1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Pairplot\n",
    "sns.pairplot(df[['cnt', 't2', 'wind_speed', 'is_holiday', 'is_weekend', 'weather_code',\n",
    "                  'season',\n",
    "                  ]],\n",
    "             hue='cnt', \n",
    "             palette='coolwarm',\n",
    "             height=3,\n",
    "             aspect=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map codes\n",
    "#Map weather code:\n",
    "weather_desc = {\n",
    "    1: 'Clear', 2: 'Scattered_Clouds', 3: 'Broken_Clouds', 4: 'Cloudy', 7: 'Rain', 10: 'Storm', 26: 'Snowfall', 94: 'Freezing_Fog'\n",
    "}\n",
    "df['weather_code'] = df['weather_code'].map(weather_desc)\n",
    "#Map season:\n",
    "seasons = {0:'Spring', 1:'Summer', 2:'Fall', 3:'Winter'}\n",
    "df['season'] = df['season'].map(seasons)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding for categorical variables\n",
    "df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month sine and cosine columns\n",
    "df['month_sin'] = np.sin(2*np.pi*df.index.month/12)\n",
    "df['month_cos'] = np.cos(2*np.pi*df.index.month/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training df till June 2016 and testing df from July 2016\n",
    "train_df = df.loc[:'2016-06-30'].copy()\n",
    "test_df = df.loc['2016-07-01':].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case some models need validation set, split train_df to get validation set. Use validation from April 2016 to June 2016. \n",
    "train_train_df = train_df.loc[:'2016-03-31'].copy()\n",
    "val_df = train_df.loc['2016-04-01':].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot seasonal decomposition\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "fig_df = train_df.copy()\n",
    "fig_df = fig_df.asfreq('D')\n",
    "fig_df.index = pd.DatetimeIndex(fig_df.index.to_timestamp())\n",
    "print(fig_df.index.freq)\n",
    "fig, axes = plt.subplots(4, 1, figsize=(20, 8))\n",
    "seasonal_decompose = seasonal_decompose(fig_df['cnt'], model='additive')\n",
    "seasonal_decompose.observed.plot(ax=axes[0], title='Observed')\n",
    "seasonal_decompose.trend.plot(ax=axes[1], title='Trend')\n",
    "seasonal_decompose.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "seasonal_decompose.resid.plot(ax=axes[3], title='Residual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clear seasonality in the data. Also, the data does not look stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(data):\n",
    "    print('Null Hypothesis: Presence of unit root (Data is not stationary)')\n",
    "    print('Alternate Hypothesis: Absence of unit root (Data is stationary)')\n",
    "    result = adfuller(data, autolag='AIC')\n",
    "    print(result)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    if result[1] > 0.05:\n",
    "        print('Data is not stationary')\n",
    "    else:\n",
    "        print('Data is stationary')\n",
    "\n",
    "check_stationarity(train_df['cnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the models to be used should be able to handle seasonality and non-stationarity. SARIMAX might be a good choice. However, there are multiple seasonalities in the data which might be difficult to capture with SARIMAX. We will use XGBoost, LSTM and Prophet for this task. Let's still try SARIMAX and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use Auto ARIMA to find the best parameter and not rely only on the ACF and PACF plots. However, ACF and PACF plots are useful to define the search space for Auto ARIMA. It is to note that Auto Arima leads to memory issue for high order models. Therefore, we will limit the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sarimax import SARIMAX_model, SARIMAX_metrics, SARIMAX_plot\n",
    "sarimax_y_train, sarimax_y_test, sarimax_y_pred = SARIMAX_model(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_test_metrics = SARIMAX_metrics(sarimax_y_test, sarimax_y_pred)\n",
    "sarimax_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot predictions\n",
    "SARIMAX_plot(sarimax_y_train, sarimax_y_test, sarimax_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from XGBoost import xgboost_model, xgboost_metrics, xgboost_plot\n",
    "xgboost_y_train, xgboost_y_test, xgboost_y_pred = xgboost_model(train_train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_metrics = xgboost_metrics(xgboost_y_test, xgboost_y_pred)\n",
    "xgb_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot predictions\n",
    "xgboost_plot(train_df, test_df, xgboost_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_encode_decode import lstm_model, lstm_metrics, lstm_plot\n",
    "lstm_y_test, lstm_y_pred = lstm_model(train_train_df,\n",
    "                                        val_df,\n",
    "                                        test_df,\n",
    "                                        lookback=28,\n",
    "                                        forecast_horizon=7,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_test_metrics = lstm_metrics(lstm_y_test, lstm_y_pred)\n",
    "lstm_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot predictions\n",
    "lstm_plot(train_df, test_df, lstm_y_pred, lookback=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Prophet import prophet_model, prophet_metrics, prophet_plot\n",
    "prophet_test_df, prophet_y_pred = prophet_model(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_test_metrics = prophet_metrics(prophet_test_df, prophet_y_pred)\n",
    "prophet_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot predictions\n",
    "prophet_plot(train_df, test_df, prophet_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results to csv\n",
    "models = ['sarimax', 'xgboost', 'lstm', 'prophet']\n",
    "metrics_dict = [sarimax_test_metrics, xgb_test_metrics, lstm_test_metrics, prophet_test_metrics]\n",
    "#Create a dataframe to store the results\n",
    "results = []\n",
    "for model, metrics in zip(models, metrics_dict):\n",
    "    results.append({'model': model, 'rmse': metrics['RMSE'], 'mae': metrics['MAE'], 'mape': metrics['MAPE'], 'r-squared': metrics['R2']})\n",
    "results = pd.DataFrame(results)\n",
    "results.to_csv('Results/metrics_bad_feature_engineering.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
