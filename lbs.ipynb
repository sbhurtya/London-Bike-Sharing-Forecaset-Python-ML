{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, GRU, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import optuna\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima import auto_arima\n",
    "from pmdarima.arima import StepwiseContext\n",
    "import os\n",
    "import holidays\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "#Save requirements\n",
    "os.system(\"pip freeze > requirements.txt\")\n",
    "\n",
    "#SEED   \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/london_merged.csv')\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metadata:\n",
    "  - \"timestamp\" - timestamp field for grouping the data\n",
    "  - \"cnt\" - the count of a new bike shares\n",
    "  - \"t1\" - real temperature in C\n",
    "  - \"t2\" - temperature in C \"feels like\"\n",
    "  - \"hum\" - humidity in percentage\n",
    "  - \"wind_speed\" - wind speed in km/h\n",
    "  - \"weather_code\" - category of the weather\n",
    "  - \"is_holiday\" - boolean field - 1 holiday / 0 non holiday\n",
    "  - \"is_weekend\" - boolean field - 1 if the day is weekend\n",
    "  - \"season\" - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter.\n",
    "  - \"weathe_code\" category description:\n",
    "     - 1 = Clear ; mostly clear but have some values with haze/fog/patches of fog/ fog in vicinity \n",
    "     - 2 = scattered clouds / few clouds \n",
    "     - 3 = Broken clouds \n",
    "     - 4 = Cloudy \n",
    "     - 7 = Rain/ light Rain shower/ Light rain \n",
    "     - 10 = rain with thunderstorm \n",
    "     - 26 = snowfall \n",
    "     - 94 = Freezing Fog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "#Keep records from 2015 and 2016\n",
    "df = df[(df['timestamp'].dt.year == 2015) | (df['timestamp'].dt.year == 2016)]\n",
    "#Sort the values by timestamp\n",
    "df = df.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No missing values. But there might be missing timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing timestamps\n",
    "all_days = pd.date_range(start=df['timestamp'].min(), end=df['timestamp'].max(), freq='h')\n",
    "missing_days = all_days[~all_days.isin(df['timestamp'])]\n",
    "print('Number of missing timestamps:', len(missing_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 130 timestamps are missing. We will imput them using existing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#London holidays\n",
    "uk_holidays = holidays.UK(years=[df['timestamp'].dt.year.min(), df['timestamp'].dt.year.max()])\n",
    "uk_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataframe using all days\n",
    "df_full = pd.DataFrame(all_days, columns=['timestamp'])\n",
    "#Merge with df to get cnt, t1, t2, hum, wind_speed, weather_code, season\n",
    "df_full = df_full.merge(df[['timestamp', 'cnt', 't1', 't2', 'hum', 'wind_speed', 'weather_code', 'season']], on='timestamp', how='left')\n",
    "#is_holiday column: 1 if holiday, 0 if not\n",
    "df_full['is_holiday'] = np.where(df_full['timestamp'].dt.date.isin(uk_holidays), 1, 0)\n",
    "df_full['is_weekend'] = np.where(df_full['timestamp'].dt.dayofweek.isin([5, 6]), 1, 0)\n",
    "\n",
    "#Backfill missing values\n",
    "df_full = df_full.ffill()\n",
    "df = df_full.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days = all_days[~all_days.isin(df['timestamp'])]\n",
    "print('Number of missing timestamps:', len(missing_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the timestamp as the index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "#Set period to 1 hour\n",
    "df.index = pd.DatetimeIndex(df.index).to_period('h')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample('D').agg({'cnt':'sum', \n",
    "                           't1':'median', \n",
    "                           't2':'median', \n",
    "                           'hum':'median', \n",
    "                           'wind_speed':'median', \n",
    "                           'weather_code': lambda x: x.value_counts().index[0], \n",
    "                           'season': lambda x: x.value_counts().index[0], \n",
    "                           'is_holiday':'max', \n",
    "                           'is_weekend':'max'})\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Boxplot of all the columns\n",
    "plt.figure(figsize=(10, 12))\n",
    "cols = df.columns\n",
    "print(cols)\n",
    "for i in range(1, len(cols)):\n",
    "    print(cols[i])\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(df[cols[i-1]])\n",
    "    plt.title(cols[i-1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no abnormal data in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Real and feels like temperature are highly correlated. Let's use feels like temperature since it is more likely to impact the decision.\n",
    "- Humidity is also correlated with temperature. We will drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop t1\n",
    "df.drop(['t1', 'hum'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Pairplot\n",
    "sns.pairplot(df[['cnt', 't2', 'wind_speed', 'is_holiday', 'is_weekend', 'weather_code',\n",
    "                  'season',\n",
    "                  ]],\n",
    "             hue='cnt', \n",
    "             palette='coolwarm',\n",
    "             height=3,\n",
    "             aspect=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map codes\n",
    "#Map weather code:\n",
    "weather_desc = {\n",
    "    1: 'Clear', 2: 'Scattered_Clouds', 3: 'Broken_Clouds', 4: 'Cloudy', 7: 'Rain', 10: 'Storm', 26: 'Snowfall', 94: 'Freezing_Fog'\n",
    "}\n",
    "df['weather_code'] = df['weather_code'].map(weather_desc)\n",
    "\n",
    "# #Map is_holiday:\n",
    "# df['is_holiday'] = df['is_holiday'].map({0:'No_Holiday', 1:'Holiday'})\n",
    "\n",
    "# #Map is_weekend:\n",
    "# df['is_weekend'] = df['is_weekend'].map({0:'Weekday', 1:'Weekend'})\n",
    "\n",
    "#Map season:\n",
    "seasons = {0:'Spring', 1:'Summer', 2:'Fall', 3:'Winter'}\n",
    "df['season'] = df['season'].map(seasons)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding for categorical variables\n",
    "df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month sine and cosine columns\n",
    "df['month_sin'] = np.sin(2*np.pi*df.index.month/12)\n",
    "df['month_cos'] = np.cos(2*np.pi*df.index.month/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training df till June 2016 and testing df from July 2016\n",
    "train_df = df.loc[:'2016-06-30'].copy()\n",
    "test_df = df.loc['2016-07-01':].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case some models need validation set, split train_df to get validation set. Use validation from April 2016 to June 2016. \n",
    "train_train_df = train_df.loc[:'2016-03-31'].copy()\n",
    "val_df = train_df.loc['2016-04-01':].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot seasonal decomposition\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "fig_df = train_df.copy()\n",
    "fig_df = fig_df.asfreq('D')\n",
    "fig_df.index = pd.DatetimeIndex(fig_df.index.to_timestamp())\n",
    "print(fig_df.index.freq)\n",
    "fig, axes = plt.subplots(4, 1, figsize=(20, 8))\n",
    "seasonal_decompose = seasonal_decompose(fig_df['cnt'], model='additive')\n",
    "seasonal_decompose.observed.plot(ax=axes[0], title='Observed')\n",
    "seasonal_decompose.trend.plot(ax=axes[1], title='Trend')\n",
    "seasonal_decompose.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "seasonal_decompose.resid.plot(ax=axes[3], title='Residual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clear seasonality in the data. Also, the data does not look stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(data):\n",
    "    print('Null Hypothesis: Presence of unit root (Data is not stationary)')\n",
    "    print('Alternate Hypothesis: Absence of unit root (Data is stationary)')\n",
    "    result = adfuller(data, autolag='AIC')\n",
    "    print(result)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('Lags: ')\n",
    "    print('p-value:', result[1])\n",
    "    if result[1] > 0.05:\n",
    "        print('Data is not stationary')\n",
    "    else:\n",
    "        print('Data is stationary')\n",
    "\n",
    "check_stationarity(train_df['cnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the models to be used should be able to handle seasonality and non-stationarity. SARIMAX might be a good choice. However, there are multiple seasonalities in the data which might be difficult to capture with SARIMAX. We will use Facebook Prophet and LSTM for this task. Let's still try SARIMAX and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use Auto ARIMA to find the best parameter and not rely only on the ACF and PACF plots. However, ACF and PACF plots are useful to define the search space for Auto ARIMA. It is to note that Auto Arima leads to memory issue for high order models. Therefore, we will limit the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = train_df.drop('cnt', axis=1)\n",
    "y_train = train_df['cnt']\n",
    "\n",
    "X_test = test_df.drop('cnt', axis=1)\n",
    "y_test = test_df['cnt']\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "feature_columns = X_train.columns\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto ARIMA\n",
    "with StepwiseContext(max_steps=3):\n",
    "    model = auto_arima(y=y_train,\n",
    "                        X=X_train_scaled,\n",
    "                        start_p=0,\n",
    "                        d=None,\n",
    "                        start_q=0,\n",
    "                        max_p=3,\n",
    "                        max_d=7,\n",
    "                        max_q=3,\n",
    "                        start_P=0,\n",
    "                        D=None,\n",
    "                        start_Q=0,\n",
    "                        m=52,\n",
    "                        max_P=5,\n",
    "                        max_D=7,\n",
    "                        max_Q=5,\n",
    "                        stationary=False,\n",
    "                        seasonal=True,\n",
    "                        stepwise = True,\n",
    "                        random=False,\n",
    "                        random_state=42,\n",
    "                        njobs=1,\n",
    "                        scoring='mae',\n",
    "                        maxiter=50,\n",
    "                        trace=True,\n",
    "                        )\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions\n",
    "y_pred = model.predict(n_periods=len(y_test), X=X_test_scaled)\n",
    "y_pred = pd.Series(y_pred, index=y_test.index)\n",
    "\n",
    "#RMSE, MAE, MAPE, R2\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('MAPE:', mape)\n",
    "print('R2:', r2)\n",
    "\n",
    "#Plot actual vs predicted\n",
    "plt.figure(figsize=(20, 6))\n",
    "# plt.plot(y_train.index.to_timestamp(), y_train, label='Train')\n",
    "plt.plot(y_test.index.to_timestamp(), y_test, label='Actual')\n",
    "plt.plot(y_test.index.to_timestamp(), y_pred, label='Predicted')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "X_train = train_train_df.drop(['cnt'], axis=1)\n",
    "y_train = train_train_df['cnt']\n",
    "\n",
    "X_val = val_df.drop(['cnt'], axis=1)\n",
    "y_val = val_df['cnt']\n",
    "\n",
    "X_test = test_df.drop(['cnt'], axis=1)\n",
    "y_test = test_df['cnt']\n",
    "\n",
    "#Standardize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost tuning using optuna\n",
    "def xgboost_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1100, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 40),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0, step=0.1),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(verbosity=1, **params)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    return mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "study_name = 'xgboost_study'\n",
    "#Delete the study if it exists\n",
    "try:\n",
    "    optuna.delete_study(study_name = study_name, storage=f'sqlite:///{study_name}.db')\n",
    "except:\n",
    "    pass\n",
    "storage = f'sqlite:///{study_name}.db'\n",
    "study = optuna.create_study(study_name=study_name, storage=storage, load_if_exists=True, sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(xgboost_objective, n_trials=100, n_jobs=5, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "best_params = study.best_params\n",
    "print('Best parameters:', best_params)\n",
    "\n",
    "#Train the model with best parameters\n",
    "model = XGBRegressor(verbosity=1, **best_params)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Predict on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "#RMSE, MAE, MAPE, R2\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "r2 = 1 - (np.sum((y_test - y_pred)**2) / np.sum((y_test - y_test.mean())**2))\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('R2:', r2)\n",
    "print('MAPE:', mape)\n",
    "\n",
    "#Plot predictions vs actual\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(train_df.index.to_timestamp(), train_df['cnt'], label='Train')\n",
    "plt.plot(test_df.index.to_timestamp(), test_df['cnt'], label='Test')\n",
    "plt.plot(test_df.index.to_timestamp(), y_pred, label='Predictions')\n",
    "plt.title('Bike Rentals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Rentals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_train_df = train_df.copy()\n",
    "pro_train_df.reset_index(inplace=True)\n",
    "pro_train_df.rename(columns={'timestamp':'ds', 'cnt':'y'}, inplace=True)\n",
    "pro_train_df['ds'] = pro_train_df['ds'].dt.to_timestamp()\n",
    "\n",
    "model = Prophet(weekly_seasonality=True, growth='flat', yearly_seasonality=True, interval_width=0.95, scaling='minmax')\n",
    "#Add holiday regressor\n",
    "model.add_country_holidays(country_name='UK')\n",
    "model.add_regressor('t2')\n",
    "model.add_regressor('wind_speed')\n",
    "model.add_regressor('is_holiday')\n",
    "model.add_regressor('is_weekend')\n",
    "model.add_regressor('weather_code_Clear')\n",
    "model.add_regressor('weather_code_Cloudy')\n",
    "model.add_regressor('weather_code_Rain')\n",
    "model.add_regressor('weather_code_Scattered_Clouds')\n",
    "model.add_regressor('weather_code_Snowfall')\n",
    "model.add_regressor('season_Spring')\n",
    "model.add_regressor('season_Summer')\n",
    "model.add_regressor('season_Winter')\n",
    "# model.add_regressor('month_sin')\n",
    "# model.add_regressor('month_cos')\n",
    "\n",
    "model.fit(pro_train_df)\n",
    "\n",
    "#Predict on test set\n",
    "pro_test_df = test_df.copy()\n",
    "pro_test_df.reset_index(inplace=True)\n",
    "pro_test_df.rename(columns={'timestamp':'ds', 'cnt':'y'}, inplace=True)\n",
    "pro_test_df['ds'] = pro_test_df['ds'].dt.to_timestamp()\n",
    "\n",
    "y_pred = model.predict(pro_test_df)\n",
    "# model.plot_components(y_pred)\n",
    "# plt.show()\n",
    "y_pred.set_index('ds', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE, MAE, R2, MAPE\n",
    "rmse = np.sqrt(mean_squared_error(test_df['cnt'], y_pred['yhat']))\n",
    "mae = mean_absolute_error(test_df['cnt'], y_pred['yhat'])\n",
    "r2 = r2_score(test_df['cnt'], y_pred['yhat'])\n",
    "mape = np.mean(np.abs((test_df.to_timestamp()['cnt'] - y_pred['yhat']) / test_df.to_timestamp()['cnt'])) * 100\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('R2:', r2)\n",
    "print('MAPE:', mape)\n",
    "\n",
    "#Plot predictions vs actual with confidence intervals\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(train_df.index.to_timestamp(), train_df['cnt'], label='Train')\n",
    "plt.plot(test_df.index.to_timestamp(), test_df['cnt'], label='Test')\n",
    "plt.plot(test_df.index.to_timestamp(), y_pred['yhat'], label='Predictions')\n",
    "plt.fill_between(test_df.index.to_timestamp(), y_pred['yhat_lower'], y_pred['yhat_upper'], color='gray', alpha=0.2)\n",
    "plt.title('Bike Rentals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Rentals')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean previous model\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "feature_columns = train_df.drop('cnt', axis=1).columns\n",
    "target_column = 'cnt'\n",
    "\n",
    "#Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "train_df_scaled = train_df.copy()\n",
    "test_df_scaled = test_df.copy()\n",
    "train_df_scaled[feature_columns] = scaler.fit_transform(train_df[feature_columns])\n",
    "test_df_scaled[feature_columns] = scaler.transform(test_df[feature_columns])\n",
    "\n",
    "X_train = train_df_scaled.drop(target_column, axis=1)\n",
    "y_train = train_df_scaled[target_column]\n",
    "\n",
    "X_test = test_df_scaled.drop(target_column, axis=1)\n",
    "y_test = test_df_scaled[target_column]\n",
    "\n",
    "lookback = 3\n",
    "forecast_horizon = 2\n",
    "\n",
    "#Create data with lookback window of 7 days and 14 days forecast\n",
    "def create_dataset(X, y, lookback=1, forecast_horizon=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - lookback - forecast_horizon + 1):\n",
    "        Xs.append(X.iloc[i:(i+lookback)].values)\n",
    "        ys.append(y.iloc[(i+lookback):(i+lookback+forecast_horizon)].values)\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_train, y_train = create_dataset(X_train, y_train, lookback, forecast_horizon)\n",
    "X_test, y_test = create_dataset(X_test, y_test, lookback, forecast_horizon)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "features = X_train.shape[2]\n",
    "\n",
    "#Tuning with optuna\n",
    "def lstm_objective(trial):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(lookback, features)))\n",
    "    model.add(LSTM(units=trial.suggest_int('units', 32, 256, step=32), activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(trial.suggest_float('dropout', 0.1, 0.5, step=0.1)))\n",
    "    model.add(LSTM(units=trial.suggest_int('units', 32, 256, step=32), activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(trial.suggest_float('dropout', 0.1, 0.5, step=0.1)))\n",
    "    model.add(Dense(forecast_horizon))\n",
    "\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-1, log=True)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=5, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "    \n",
    "    return np.min(history.history['val_loss'])\n",
    "\n",
    "study_name = 'lstm_study'\n",
    "#Delete the study if it exists\n",
    "try:\n",
    "    optuna.delete_study(study_name = study_name, storage=f'sqlite:///{study_name}.db')\n",
    "except:\n",
    "    pass\n",
    "storage = f'sqlite:///{study_name}.db'\n",
    "study = optuna.create_study(study_name=study_name, storage=storage, load_if_exists=True, sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(lstm_objective, n_trials=50, n_jobs=5, show_progress_bar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "best_params = study.best_params\n",
    "print('Best parameters:', best_params)\n",
    "\n",
    "#Train the model with best parameters\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(lookback, features)))\n",
    "model.add(LSTM(units=best_params['units'], activation='relu', return_sequences=True))\n",
    "model.add(Dropout(best_params['dropout']))\n",
    "model.add(LSTM(units=best_params['units'], activation='relu', return_sequences=False))\n",
    "model.add(Dropout(best_params['dropout']))\n",
    "model.add(Dense(forecast_horizon))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2100, min_delta=5, restore_best_weights=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train.reshape(y_train.shape[0], -1),\n",
    "    epochs=400,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    callbacks=[es]\n",
    ")\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.reshape(predictions.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE, MAE, R2, MAPE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('R2:', r2)\n",
    "print('MAPE:', mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot predictions vs actual\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(train_df.index[lookback:].strftime('%Y-%m-%d'), train_df['cnt'][lookback:], label='Train')\n",
    "plt.plot(test_df.index[lookback:].strftime('%Y-%m-%d'), test_df['cnt'][lookback:], label='Actual')\n",
    "plt.plot(test_df.index[lookback+1:].strftime('%Y-%m-%d'), predictions[:, 0], label='Predictions')\n",
    "plt.title('Bike Rentals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Rentals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "#Clean previous model\n",
    "tf.compat.v1.reset_default_graph\n",
    "\n",
    "feature_columns = train_df.drop('cnt', axis=1).columns\n",
    "target_column = 'cnt'\n",
    "\n",
    "#Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "train_df_scaled = train_train_df.copy()\n",
    "val_df_scaled = val_df.copy()\n",
    "test_df_scaled = test_df.copy()\n",
    "train_df_scaled[feature_columns] = scaler.fit_transform(train_df_scaled[feature_columns])\n",
    "val_df_scaled[feature_columns] = scaler.transform(val_df_scaled[feature_columns])\n",
    "test_df_scaled[feature_columns] = scaler.transform(test_df_scaled[feature_columns])\n",
    "\n",
    "def create_dataset(df, n_deterministic_features, window_size, forecast_window, batch_size):\n",
    "    size = window_size + forecast_window\n",
    "\n",
    "    data = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "\n",
    "    data = data.window(size, shift=1, drop_remainder=True)\n",
    "    data = data.flat_map(lambda x: x.batch(size))\n",
    "\n",
    "    data = data.shuffle(int(len(df)/2), seed=42)\n",
    "    data = data.map(lambda x: ((x[:-forecast_window],\n",
    "                                  x[-forecast_window:, n_deterministic_features:]),\n",
    "                                  x[-forecast_window:,0]))\n",
    "    \n",
    "    data = data.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return data\n",
    "\n",
    "lookback = 14 #days\n",
    "forecast_horizon = 7 #day\n",
    "\n",
    "number_total_features = len(train_df.columns)\n",
    "number_aleatoric_features = 1 #Only cnt is aleatoric\n",
    "number_deterministic_features = number_total_features - number_aleatoric_features\n",
    "\n",
    "batch_size = 32\n",
    "training_window = create_dataset(train_df_scaled,\n",
    "                                 number_deterministic_features,\n",
    "                                 lookback,\n",
    "                                 forecast_horizon,\n",
    "                                 batch_size)\n",
    "\n",
    "validation_window = create_dataset(val_df_scaled,\n",
    "                                   number_deterministic_features,\n",
    "                                   lookback,\n",
    "                                   forecast_horizon,\n",
    "                                   batch_size)\n",
    "\n",
    "testing_window = create_dataset(test_df_scaled,\n",
    "                                number_deterministic_features,\n",
    "                                lookback,\n",
    "                                forecast_horizon,\n",
    "                                batch_size)  \n",
    "\n",
    "\n",
    "dim = 32\n",
    "past_inputs = Input(shape=(lookback, number_total_features), name='past_inputs')\n",
    "encoder = LSTM(dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(past_inputs)\n",
    "\n",
    "future_inputs = Input(shape=(forecast_horizon, number_deterministic_features), name='future_inputs')\n",
    "decoder = LSTM(dim, return_sequences=True)\n",
    "\n",
    "print(past_inputs.shape, future_inputs.shape)\n",
    "\n",
    "x = decoder(future_inputs, initial_state=[state_h, state_c])\n",
    "x = Dense(16, activation='relu')(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "output = Dense(1, activation='relu')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[past_inputs, future_inputs], outputs=output)\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(training_window, epochs=50, validation_data=validation_window, verbose=1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
