{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='-1'\n",
    "os.environ['TF_CUDNN_USE_AUTOTUNE'] ='0'\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "#SEED\n",
    "def seed_everything():\n",
    "    tf.random.set_seed(42)\n",
    "    tf.keras.utils.set_random_seed(42)\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    K.set_session(sess)\n",
    "    K.clear_session()\n",
    "from prophet import Prophet\n",
    "import optuna\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima import auto_arima\n",
    "from pmdarima.arima import StepwiseContext\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import holidays\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "#Save requirements\n",
    "os.system(\"pip freeze > requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>cnt</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>hum</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_code</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04 00:00:00</td>\n",
       "      <td>182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-04 01:00:00</td>\n",
       "      <td>138</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  cnt   t1   t2   hum  wind_speed  weather_code  \\\n",
       "0  2015-01-04 00:00:00  182  3.0  2.0  93.0         6.0           3.0   \n",
       "1  2015-01-04 01:00:00  138  3.0  2.5  93.0         5.0           1.0   \n",
       "\n",
       "   is_holiday  is_weekend  season  \n",
       "0         0.0         1.0     3.0  \n",
       "1         0.0         1.0     3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/london_merged.csv')\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metadata:\n",
    "  - \"timestamp\" - timestamp field for grouping the data\n",
    "  - \"cnt\" - the count of a new bike shares\n",
    "  - \"t1\" - real temperature in C\n",
    "  - \"t2\" - temperature in C \"feels like\"\n",
    "  - \"hum\" - humidity in percentage\n",
    "  - \"wind_speed\" - wind speed in km/h\n",
    "  - \"weather_code\" - category of the weather\n",
    "  - \"is_holiday\" - boolean field - 1 holiday / 0 non holiday\n",
    "  - \"is_weekend\" - boolean field - 1 if the day is weekend\n",
    "  - \"season\" - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter.\n",
    "  - \"weathe_code\" category description:\n",
    "     - 1 = Clear ; mostly clear but have some values with haze/fog/patches of fog/ fog in vicinity \n",
    "     - 2 = scattered clouds / few clouds \n",
    "     - 3 = Broken clouds \n",
    "     - 4 = Cloudy \n",
    "     - 7 = Rain/ light Rain shower/ Light rain \n",
    "     - 10 = rain with thunderstorm \n",
    "     - 26 = snowfall \n",
    "     - 94 = Freezing Fog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "#Keep records from 2015 and 2016\n",
    "df = df[(df['timestamp'].dt.year == 2015) | (df['timestamp'].dt.year == 2016)]\n",
    "#Sort the values by timestamp\n",
    "df = df.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No missing values. But there might be missing timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing timestamps\n",
    "all_days = pd.date_range(start=df['timestamp'].min(), end=df['timestamp'].max(), freq='h')\n",
    "missing_days = all_days[~all_days.isin(df['timestamp'])]\n",
    "print('Number of missing timestamps:', len(missing_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 130 timestamps are missing. We will imput them using existing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#London holidays\n",
    "uk_holidays = holidays.UK(years=[df['timestamp'].dt.year.min(), df['timestamp'].dt.year.max()])\n",
    "uk_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataframe using all days\n",
    "df_full = pd.DataFrame(all_days, columns=['timestamp'])\n",
    "#Merge with df to get cnt, t1, t2, hum, wind_speed, weather_code, season\n",
    "df_full = df_full.merge(df[['timestamp', 'cnt', 't1', 't2', 'hum', 'wind_speed', 'weather_code', 'season']], on='timestamp', how='left')\n",
    "#is_holiday column: 1 if holiday, 0 if not\n",
    "df_full['is_holiday'] = np.where(df_full['timestamp'].dt.date.isin(uk_holidays), 1, 0)\n",
    "df_full['is_weekend'] = np.where(df_full['timestamp'].dt.dayofweek.isin([5, 6]), 1, 0)\n",
    "\n",
    "#Backfill missing values\n",
    "df_full = df_full.ffill()\n",
    "df = df_full.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days = all_days[~all_days.isin(df['timestamp'])]\n",
    "print('Number of missing timestamps:', len(missing_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the timestamp as the index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "#Set period to 1 hour\n",
    "df.index = pd.DatetimeIndex(df.index).to_period('h')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample('D').agg({'cnt':'sum', \n",
    "                           't1':'median', \n",
    "                           't2':'median', \n",
    "                           'hum':'median', \n",
    "                           'wind_speed':'median', \n",
    "                           'weather_code': lambda x: x.value_counts().index[0], \n",
    "                           'season': lambda x: x.value_counts().index[0], \n",
    "                           'is_holiday':'max', \n",
    "                           'is_weekend':'max'})\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Boxplot of all the columns\n",
    "plt.figure(figsize=(10, 12))\n",
    "cols = df.columns\n",
    "print(cols)\n",
    "for i in range(1, len(cols)):\n",
    "    print(cols[i])\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(df[cols[i-1]])\n",
    "    plt.title(cols[i-1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no abnormal data in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Real and feels like temperature are highly correlated. Let's use feels like temperature since it is more likely to impact the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop t1\n",
    "df.drop(['t1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Pairplot\n",
    "sns.pairplot(df[['cnt', 't2', 'wind_speed', 'is_holiday', 'is_weekend', 'weather_code',\n",
    "                  'season',\n",
    "                  ]],\n",
    "             hue='cnt', \n",
    "             palette='coolwarm',\n",
    "             height=3,\n",
    "             aspect=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map codes\n",
    "#Map weather code:\n",
    "weather_desc = {\n",
    "    1: 'Clear', 2: 'Scattered_Clouds', 3: 'Broken_Clouds', 4: 'Cloudy', 7: 'Rain', 10: 'Storm', 26: 'Snowfall', 94: 'Freezing_Fog'\n",
    "}\n",
    "df['weather_code'] = df['weather_code'].map(weather_desc)\n",
    "#Map season:\n",
    "seasons = {0:'Spring', 1:'Summer', 2:'Fall', 3:'Winter'}\n",
    "df['season'] = df['season'].map(seasons)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding for categorical variables\n",
    "df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month sine and cosine columns\n",
    "df['month_sin'] = np.sin(2*np.pi*df.index.month/12)\n",
    "df['month_cos'] = np.cos(2*np.pi*df.index.month/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training df till June 2016 and testing df from July 2016\n",
    "train_df = df.loc[:'2016-06-30'].copy()\n",
    "test_df = df.loc['2016-07-01':].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case some models need validation set, split train_df to get validation set. Use validation from April 2016 to June 2016. \n",
    "train_train_df = train_df.loc[:'2016-03-31'].copy()\n",
    "val_df = train_df.loc['2016-04-01':].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot seasonal decomposition\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "fig_df = train_df.copy()\n",
    "fig_df = fig_df.asfreq('D')\n",
    "fig_df.index = pd.DatetimeIndex(fig_df.index.to_timestamp())\n",
    "print(fig_df.index.freq)\n",
    "fig, axes = plt.subplots(4, 1, figsize=(20, 8))\n",
    "seasonal_decompose = seasonal_decompose(fig_df['cnt'], model='additive')\n",
    "seasonal_decompose.observed.plot(ax=axes[0], title='Observed')\n",
    "seasonal_decompose.trend.plot(ax=axes[1], title='Trend')\n",
    "seasonal_decompose.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "seasonal_decompose.resid.plot(ax=axes[3], title='Residual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clear seasonality in the data. Also, the data does not look stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(data):\n",
    "    print('Null Hypothesis: Presence of unit root (Data is not stationary)')\n",
    "    print('Alternate Hypothesis: Absence of unit root (Data is stationary)')\n",
    "    result = adfuller(data, autolag='AIC')\n",
    "    print(result)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    if result[1] > 0.05:\n",
    "        print('Data is not stationary')\n",
    "    else:\n",
    "        print('Data is stationary')\n",
    "\n",
    "check_stationarity(train_df['cnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the models to be used should be able to handle seasonality and non-stationarity. SARIMAX might be a good choice. However, there are multiple seasonalities in the data which might be difficult to capture with SARIMAX. We will use XGBoost, LSTM and Prophet for this task. Let's still try SARIMAX and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use Auto ARIMA to find the best parameter and not rely only on the ACF and PACF plots. However, ACF and PACF plots are useful to define the search space for Auto ARIMA. It is to note that Auto Arima leads to memory issue for high order models. Therefore, we will limit the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data\n",
    "scaler = MinMaxScaler().set_output(transform=\"pandas\")\n",
    "X_train = train_df.drop('cnt', axis=1)\n",
    "y_train = train_df['cnt']\n",
    "\n",
    "X_test = test_df.drop('cnt', axis=1)\n",
    "y_test = test_df['cnt']\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "feature_columns = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto ARIMA\n",
    "with StepwiseContext(max_steps=3):\n",
    "    model = auto_arima(y=y_train,\n",
    "                        X=X_train_scaled,\n",
    "                        start_p=0,\n",
    "                        d=None,\n",
    "                        start_q=0,\n",
    "                        max_p=3,\n",
    "                        max_d=7,\n",
    "                        max_q=3,\n",
    "                        start_P=0,\n",
    "                        D=None,\n",
    "                        start_Q=0,\n",
    "                        m=52,\n",
    "                        max_P=5,\n",
    "                        max_D=7,\n",
    "                        max_Q=5,\n",
    "                        stationary=False,\n",
    "                        seasonal=True,\n",
    "                        stepwise = True,\n",
    "                        random=False,\n",
    "                        random_state=42,\n",
    "                        njobs=1,\n",
    "                        scoring='mse',\n",
    "                        maxiter=50,\n",
    "                        trace=True,\n",
    "                        )\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions\n",
    "y_pred = model.predict(n_periods=len(y_test), X=X_test_scaled)\n",
    "y_pred = pd.Series(y_pred, index=y_test.index)\n",
    "\n",
    "#RMSE, MAE, MAPE, R2\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('MAPE:', mape)\n",
    "print('R2:', r2)\n",
    "\n",
    "#Plot actual vs predicted\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(y_train.index.to_timestamp(), y_train, label='Train')\n",
    "plt.plot(y_test.index.to_timestamp(), y_test, label='Actual')\n",
    "plt.plot(y_test.index.to_timestamp(), y_pred, label='Predicted')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "X_train = train_train_df.drop(['cnt'], axis=1)\n",
    "y_train = train_train_df['cnt']\n",
    "\n",
    "X_val = val_df.drop(['cnt'], axis=1)\n",
    "y_val = val_df['cnt']\n",
    "\n",
    "X_test = test_df.drop(['cnt'], axis=1)\n",
    "y_test = test_df['cnt']\n",
    "\n",
    "#Standardize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost tuning using optuna\n",
    "def xgboost_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1100, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 40),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0, step=0.1),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(early_stopping_rounds=10, eval_metric=mean_squared_error, **params)\n",
    "    model.fit(X_train_scaled, y_train, \n",
    "              eval_set=[(X_val_scaled, y_val)],\n",
    "              verbose=False)\n",
    "    \n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    return mse\n",
    "    \n",
    "study_name = 'xgboost_study'\n",
    "#Delete the study if it exists\n",
    "try:\n",
    "    optuna.delete_study(study_name = study_name, storage=f'sqlite:///{study_name}.db')\n",
    "except:\n",
    "    pass\n",
    "storage = f'sqlite:///{study_name}.db'\n",
    "study = optuna.create_study(study_name=study_name, storage=storage, load_if_exists=True, sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(xgboost_objective, n_trials=50, n_jobs=1, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "best_params = study.best_params\n",
    "print('Best parameters:', best_params)\n",
    "\n",
    "#Train the model with best parameters\n",
    "model = XGBRegressor(early_stopping_rounds=10, eval_metric=mean_squared_error, **best_params)\n",
    "model.fit(X_train_scaled, y_train, \n",
    "            eval_set=[(X_val_scaled, y_val)],\n",
    "            verbose=False)\n",
    "\n",
    "#Predict on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "#RMSE, MAE, MAPE, R2\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "r2 = 1 - (np.sum((y_test - y_pred)**2) / np.sum((y_test - y_test.mean())**2))\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('R2:', r2)\n",
    "print('MAPE:', mape)\n",
    "\n",
    "#Plot predictions vs actual\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(train_df.index.to_timestamp(), train_df['cnt'], label='Train')\n",
    "plt.plot(test_df.index.to_timestamp(), test_df['cnt'], label='Test')\n",
    "plt.plot(test_df.index.to_timestamp(), y_pred, label='Predictions')\n",
    "plt.title('Bike Rentals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Rentals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_train_df = train_df.copy()\n",
    "pro_train_df.reset_index(inplace=True)\n",
    "pro_train_df.rename(columns={'timestamp':'ds', 'cnt':'y'}, inplace=True)\n",
    "pro_train_df['ds'] = pro_train_df['ds'].dt.to_timestamp()\n",
    "\n",
    "model = Prophet(weekly_seasonality=True, growth='flat', yearly_seasonality=True, interval_width=0.95, scaling='minmax')\n",
    "#Add holiday regressor\n",
    "model.add_country_holidays(country_name='UK')\n",
    "reg_cols = train_df.drop(['cnt'], axis=1).columns\n",
    "for col in reg_cols:\n",
    "    model.add_regressor(col)\n",
    "\n",
    "model.fit(pro_train_df)\n",
    "\n",
    "#Predict on test set\n",
    "pro_test_df = test_df.copy()\n",
    "pro_test_df.reset_index(inplace=True)\n",
    "pro_test_df.rename(columns={'timestamp':'ds', 'cnt':'y'}, inplace=True)\n",
    "pro_test_df['ds'] = pro_test_df['ds'].dt.to_timestamp()\n",
    "\n",
    "y_pred = model.predict(pro_test_df)\n",
    "model.plot_components(y_pred)\n",
    "plt.show()\n",
    "y_pred.set_index('ds', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE, MAE, R2, MAPE\n",
    "rmse = np.sqrt(mean_squared_error(test_df['cnt'], y_pred['yhat']))\n",
    "mae = mean_absolute_error(test_df['cnt'], y_pred['yhat'])\n",
    "r2 = r2_score(test_df['cnt'], y_pred['yhat'])\n",
    "mape = np.mean(np.abs((test_df.to_timestamp()['cnt'] - y_pred['yhat']) / test_df.to_timestamp()['cnt'])) * 100\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('R2:', r2)\n",
    "print('MAPE:', mape)\n",
    "\n",
    "#Plot predictions vs actual with confidence intervals\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(train_df.index.to_timestamp(), train_df['cnt'], label='Train')\n",
    "plt.plot(test_df.index.to_timestamp(), test_df['cnt'], label='Test')\n",
    "plt.plot(test_df.index.to_timestamp(), y_pred['yhat'], label='Predictions')\n",
    "plt.fill_between(test_df.index.to_timestamp(), y_pred['yhat_lower'], y_pred['yhat_upper'], color='gray', alpha=0.2)\n",
    "plt.title('Bike Rentals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Rentals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = train_df.drop('cnt', axis=1).columns\n",
    "target_column = 'cnt'\n",
    "\n",
    "#Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "train_df_scaled = train_train_df.copy()\n",
    "val_df_scaled = val_df.copy()\n",
    "test_df_scaled = test_df.copy()\n",
    "train_df_scaled[feature_columns] = scaler.fit_transform(train_df_scaled[feature_columns])\n",
    "val_df_scaled[feature_columns] = scaler.transform(val_df_scaled[feature_columns])\n",
    "test_df_scaled[feature_columns] = scaler.transform(test_df_scaled[feature_columns])\n",
    "\n",
    "def create_dataset(df, n_deterministic_features,\n",
    "                   window_size, forecast_size,\n",
    "                   batch_size):\n",
    "    total_size = window_size + forecast_size\n",
    "\n",
    "    data = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "    data = data.window(total_size, shift=1, drop_remainder=True)\n",
    "    data = data.flat_map(lambda k: k.batch(total_size))\n",
    "    # data = data.shuffle(shuffle_buffer_size, seed=42)\n",
    "    data = data.map(lambda k: ((k[:-forecast_size],\n",
    "                                k[-forecast_size:, -n_deterministic_features:]),\n",
    "                               k[-forecast_size:, 0]))\n",
    "\n",
    "    return data.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "lookback = 28 #days\n",
    "forecast_horizon = 7 #day\n",
    "\n",
    "number_of_features = len(train_df.columns)\n",
    "number_of_aleatoric_features = 1 #Only cnt is aleatoric\n",
    "number_of_deterministic_features = number_of_features - number_of_aleatoric_features\n",
    "\n",
    "batch_size = 1\n",
    "training_window = create_dataset(train_df_scaled,\n",
    "                                 number_of_deterministic_features,\n",
    "                                 lookback,\n",
    "                                 forecast_horizon,\n",
    "                                 batch_size)\n",
    "\n",
    "validation_window = create_dataset(val_df_scaled,\n",
    "                                   number_of_deterministic_features,\n",
    "                                   lookback,\n",
    "                                   forecast_horizon,\n",
    "                                   batch_size)\n",
    "\n",
    "testing_window = create_dataset(test_df_scaled,\n",
    "                                number_of_deterministic_features,\n",
    "                                lookback,\n",
    "                                forecast_horizon,\n",
    "                                batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune using optuna\n",
    "def lstm_objective(trial):\n",
    "    seed_everything()\n",
    "    latent_dim = trial.suggest_categorical('latent_dim', [16, 32])\n",
    "    num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])\n",
    "    dense_units = trial.suggest_categorical('dense_units', [32, 64, 128, 256])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 5e-4, 1e-2, log=True)\n",
    "\n",
    "    past_inputs = tf.keras.Input(\n",
    "        shape=(lookback, number_of_features), name='past_inputs')\n",
    "    # Encoding the past\n",
    "    encoder = tf.keras.layers.LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(past_inputs)\n",
    "\n",
    "    future_inputs = tf.keras.Input(\n",
    "        shape=(forecast_horizon, number_of_deterministic_features), name='future_inputs')\n",
    "    \n",
    "    decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True)\n",
    "    x = decoder_lstm(future_inputs,\n",
    "                    initial_state=[state_h, state_c])\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    for _ in range(num_layers - 1):\n",
    "        x = tf.keras.layers.LSTM(latent_dim, return_sequences=True)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(int(dense_units/2), activation='relu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, activation='relu')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[past_inputs, future_inputs], outputs=output)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                    loss='mse',\n",
    "                    metrics=['mape'])\n",
    "    \n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                          mode='min', \n",
    "                                          verbose=0, \n",
    "                                          patience=10, \n",
    "                                          restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(training_window, \n",
    "                        epochs=100, \n",
    "                        validation_data=validation_window, \n",
    "                        callbacks=[es],\n",
    "                        verbose=0,\n",
    "                        shuffle=False\n",
    "                    )\n",
    "    return np.min(history.history['val_loss'])\n",
    "\n",
    "study_name = 'lstm_study'\n",
    "#Delete the study if it exists\n",
    "try:\n",
    "    optuna.delete_study(study_name = study_name, storage=f'sqlite:///{study_name}.db')\n",
    "except:\n",
    "    pass\n",
    "storage = f'sqlite:///{study_name}.db'\n",
    "study = optuna.create_study(study_name=study_name, storage=storage, load_if_exists=True, sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(lstm_objective, n_trials=5, n_jobs=1, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "study_name = 'lstm_study'\n",
    "#Load the study\n",
    "study = optuna.load_study(study_name=study_name, storage=f'sqlite:///{study_name}.db')\n",
    "best_params = study.best_params\n",
    "seed_everything()\n",
    "\n",
    "\n",
    "#Train the model with best parameters\n",
    "latent_dim = best_params['latent_dim']\n",
    "num_layers = best_params['num_layers']\n",
    "dense_units = best_params['dense_units']\n",
    "dropout = best_params['dropout']\n",
    "learning_rate = best_params['learning_rate']\n",
    "\n",
    "past_inputs = tf.keras.Input(\n",
    "    shape=(lookback, number_of_features), name='past_inputs')\n",
    "# Encoding the past\n",
    "encoder = tf.keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(past_inputs)\n",
    "\n",
    "future_inputs = tf.keras.Input(\n",
    "    shape=(forecast_horizon, number_of_deterministic_features), name='future_inputs')\n",
    "\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True)\n",
    "x = decoder_lstm(future_inputs,\n",
    "                initial_state=[state_h, state_c])\n",
    "x = tf.keras.layers.Dropout(dropout)(x)\n",
    "for _ in range(num_layers - 1):\n",
    "    x = tf.keras.layers.LSTM(latent_dim, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(dense_units, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(int(dense_units/2), activation='relu')(x)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation='relu')(x)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=[past_inputs, future_inputs], outputs=output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                loss='mse',\n",
    "                metrics=['mape'])\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                        mode='min', \n",
    "                                        verbose=0, \n",
    "                                        patience=20, \n",
    "                                        restore_best_weights=True)\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "history = model.fit(training_window, \n",
    "                    epochs=300, \n",
    "                    validation_data=validation_window, \n",
    "                    callbacks=[es],\n",
    "                    verbose=0,\n",
    "                    shuffle=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "actuals = []\n",
    "for i, data in enumerate(testing_window):\n",
    "    if i % 7 == 0:\n",
    "        (past, future), truth = data\n",
    "        y_pred = model.predict([past, future], verbose=0)\n",
    "        y_pred = y_pred.flatten()\n",
    "        truth = truth.numpy().flatten()\n",
    "        predictions.append(y_pred)\n",
    "        actuals.append(truth)\n",
    "\n",
    "predictions = [item for sublist in predictions for item in sublist]\n",
    "actuals = [item for sublist in actuals for item in sublist]\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "#RMSE, MAE, MAPE, R2\n",
    "rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n",
    "r2 = r2_score(actuals, predictions)\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('MAPE:', mape)\n",
    "print('R2:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred_df = test_df[lookback:lookback+154].copy()\n",
    "lstm_pred_df['predictions'] = predictions\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(train_df.index.to_timestamp(), train_df['cnt'], label='Train')\n",
    "plt.plot(lstm_pred_df.index.to_timestamp(), lstm_pred_df['cnt'], label='Actual')\n",
    "plt.plot(lstm_pred_df.index.to_timestamp(), lstm_pred_df['predictions'], label='Predictions')\n",
    "plt.plot(test_df.head(lookback).index.to_timestamp(), test_df.head(lookback)['cnt'], label='Test Lookback Period')\n",
    "plt.title('Bike Rentals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Rentals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
