{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import optuna\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "#SEED   \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/london_merged.csv')\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metadata:\n",
    "  - \"timestamp\" - timestamp field for grouping the data\n",
    "  - \"cnt\" - the count of a new bike shares\n",
    "  - \"t1\" - real temperature in C\n",
    "  - \"t2\" - temperature in C \"feels like\"\n",
    "  - \"hum\" - humidity in percentage\n",
    "  - \"wind_speed\" - wind speed in km/h\n",
    "  - \"weather_code\" - category of the weather\n",
    "  - \"is_holiday\" - boolean field - 1 holiday / 0 non holiday\n",
    "  - \"is_weekend\" - boolean field - 1 if the day is weekend\n",
    "  - \"season\" - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter.\n",
    "  - \"weathe_code\" category description:\n",
    "     - 1 = Clear ; mostly clear but have some values with haze/fog/patches of fog/ fog in vicinity \n",
    "     - 2 = scattered clouds / few clouds \n",
    "     - 3 = Broken clouds \n",
    "     - 4 = Cloudy \n",
    "     - 7 = Rain/ light Rain shower/ Light rain \n",
    "     - 10 = rain with thunderstorm \n",
    "     - 26 = snowfall \n",
    "     - 94 = Freezing Fog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "#Sort the values by timestamp\n",
    "df = df.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No missing values. But there might be missing timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing timestamps\n",
    "all_days = pd.date_range(start=df['timestamp'].min(), end=df['timestamp'].max(), freq='h')\n",
    "missing_days = all_days[~all_days.isin(df['timestamp'])]\n",
    "print('Number of missing timestamps:', len(missing_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 130 timestamps are missing. We will imput them using existing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#London holidays\n",
    "import holidays\n",
    "uk_holidays = holidays.UK(years=[df['timestamp'].dt.year.min(), df['timestamp'].dt.year.max()])\n",
    "uk_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataframe using all days\n",
    "df_full = pd.DataFrame(all_days, columns=['timestamp'])\n",
    "#Merge with df to get cnt, t1, t2, hum, wind_speed, weather_code, season\n",
    "df_full = df_full.merge(df[['timestamp', 'cnt', 't1', 't2', 'hum', 'wind_speed', 'weather_code', 'season']], on='timestamp', how='left')\n",
    "#is_holiday column: 1 if holiday, 0 if not\n",
    "df_full['is_holiday'] = np.where(df_full['timestamp'].dt.date.isin(uk_holidays), 1, 0)\n",
    "df_full['is_weekend'] = np.where(df_full['timestamp'].dt.dayofweek.isin([5, 6]), 1, 0)\n",
    "\n",
    "#Backfill missing values\n",
    "df_full = df_full.ffill()\n",
    "df = df_full.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days = all_days[~all_days.isin(df['timestamp'])]\n",
    "print('Number of missing timestamps:', len(missing_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the timestamp as the index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "#Set period to 1 hour\n",
    "df.index = pd.DatetimeIndex(df.index).to_period('h')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of all the columns\n",
    "plt.figure(figsize=(10, 12))\n",
    "cols = df.columns\n",
    "for i in range(1, len(cols)):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(df[cols[i]])\n",
    "    plt.title(cols[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no abnormal data in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Real and feels like temperature are highly correlated. Let's use feels like temperature since it is more likely to impact the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop t1\n",
    "df.drop('t1', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map codes\n",
    "#Map weather code:\n",
    "weather_desc = {\n",
    "    1: 'Clear', 2: 'Scattered_Clouds', 3: 'Broken_Clouds', 4: 'Cloudy', 7: 'Rain', 10: 'Storm', 26: 'Snowfall', 94: 'Freezing_Fog'\n",
    "}\n",
    "df['weather_code'] = df['weather_code'].map(weather_desc)\n",
    "\n",
    "#Map is_holiday:\n",
    "df['is_holiday'] = df['is_holiday'].map({0:'No_Holiday', 1:'Holiday'})\n",
    "\n",
    "#Map is_weekend:\n",
    "df['is_weekend'] = df['is_weekend'].map({0:'Weekday', 1:'Weekend'})\n",
    "\n",
    "#Map season:\n",
    "seasons = {0:'Spring', 1:'Summer', 2:'Fall', 3:'Winter'}\n",
    "df['season'] = df['season'].map(seasons)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding for categorical variables\n",
    "df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train and test\n",
    "X = df.drop('cnt', axis=1)\n",
    "y = df['cnt']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
    "\n",
    "#Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot to check for stationarity\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_train.to_timestamp())\n",
    "plt.title('Hourly Bike Rentals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no noticeable trend in the data. The data looks stationary. Let's verify this with the Augmented Dickey-Fuller test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(data):\n",
    "    print('Null Hypothesis: Presence of unit root (Data is not stationary)')\n",
    "    print('Alternate Hypothesis: Absence of unit root (Data is stationary)')\n",
    "    result = adfuller(data, autolag='AIC')\n",
    "    print(result)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    if result[1] > 0.05:\n",
    "        print('Data is not stationary')\n",
    "    else:\n",
    "        print('Data is stationary')\n",
    "\n",
    "#Check stationarity of the target variable\n",
    "check_stationarity(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train.to_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACF and PACF plots\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(211)\n",
    "plot_acf(y_train, lags=168, ax=plt.gca())\n",
    "plt.subplot(212)\n",
    "plot_pacf(y_train, lags=168, ax=plt.gca())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Estimating AR terms: The PACF plot shows a significant spike at lag 1, 2, 3, 4, 5 and 6. Therefore, p = 6.\n",
    "- Estimating I term: The ADF test shows that the data is already stationary. Therefore, d = 0.\n",
    "- Estimating MA terms: The ACF plot shows a significant spike at lag 1, 2, 3, and 4. Therefore, q = 4.\n",
    "- Estimating seasonal AR terms: The PACF plot shows a significant spike at lag 24. Therefore, P = 1.\n",
    "- Estimating seasonal I term: The ADF test shows that the data is already stationary. Therefore, D = 0.\n",
    "- Estimating seasonal MA terms: The ACF plot shows a significant spike at eight 24 lag intervals. Let's start Q with 8.\n",
    "- ACF plot shows strong correlation at lag 24. This indicates a daily seasonality. Therefore, s = 24.\n",
    "\n",
    "However, running the model with these parameters is computationally expensive. For this reason, as an example, I will use small order values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model with lookback of 24 hours*7 days\n",
    "lookup_hours = 24*7\n",
    "forecast_horizon = 24\n",
    "def create_dataset(X, y, lookback):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - lookback - forecast_horizon):\n",
    "        Xs.append(X[i:(i + lookback)])\n",
    "        ys.append(y[i + lookback:i + lookback + forecast_horizon])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_train, y_train = create_dataset(X_train, y_train, lookup_hours)\n",
    "X_test, y_test = create_dataset(X_test, y_test, lookup_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "# Tuning the LSTM model using Optuna\n",
    "def objective(trial):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(units=trial.suggest_int('units', 32, 512)))\n",
    "    model.add(Dropout(trial.suggest_float('dropout', 0.2, 0.5)))\n",
    "    model.add(Dense(forecast_horizon))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, \n",
    "                        callbacks=[EarlyStopping('val_loss', patience=5)], verbose=0)\n",
    "    \n",
    "    return np.min(history.history['val_loss'])\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
    "trial = study.best_trial\n",
    "print('Loss: ', trial.value)\n",
    "print('Best hyperparameters: ', trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
